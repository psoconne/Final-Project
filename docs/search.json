[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA of Diabetes Health Indicators Dataset",
    "section": "",
    "text": "The data used in these analyses are the results of a yearly survey that collects health-related data on Americans. The majority of the variables in this data set are binary (yes/no). Their are some variables that have more than two levels including GenHlth, Age, Education, and Income. The only variables with a numeric data type are BMI, MentHlth, and PhysHlth.\nThe purpose of our exploratory data analysis is to ensure that all data types are correct, so any variables that need to be converted to factor data type will be adjusted here. It is also good practice to determine the rate of missing values in an EDA, so if there are any missing values they can be dealt with now instead of causing issues in the modeling phase.\nOur response variable will ultimately be Diabetes_binary, which has responses of 0 = no diabetes and 1 = diabetes. The goal of modeling will be to see which of the other variables are good at predicting diabetes."
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "EDA of Diabetes Health Indicators Dataset",
    "section": "",
    "text": "The data used in these analyses are the results of a yearly survey that collects health-related data on Americans. The majority of the variables in this data set are binary (yes/no). Their are some variables that have more than two levels including GenHlth, Age, Education, and Income. The only variables with a numeric data type are BMI, MentHlth, and PhysHlth.\nThe purpose of our exploratory data analysis is to ensure that all data types are correct, so any variables that need to be converted to factor data type will be adjusted here. It is also good practice to determine the rate of missing values in an EDA, so if there are any missing values they can be dealt with now instead of causing issues in the modeling phase.\nOur response variable will ultimately be Diabetes_binary, which has responses of 0 = no diabetes and 1 = diabetes. The goal of modeling will be to see which of the other variables are good at predicting diabetes."
  },
  {
    "objectID": "EDA.html#data",
    "href": "EDA.html#data",
    "title": "EDA of Diabetes Health Indicators Dataset",
    "section": "Data",
    "text": "Data\n\nUse a relative path to import the data.\n\n\nlibrary(tidyverse)\n\n\ndiabetes_data &lt;- read_csv('diabetes_binary_health_indicators_BRFSS2015.csv')\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstr(diabetes_data)\n\nspc_tbl_ [253,680 × 22] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Diabetes_binary     : num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ HighBP              : num [1:253680] 1 0 1 1 1 1 1 1 1 0 ...\n $ HighChol            : num [1:253680] 1 0 1 0 1 1 0 1 1 0 ...\n $ CholCheck           : num [1:253680] 1 0 1 1 1 1 1 1 1 1 ...\n $ BMI                 : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : num [1:253680] 1 1 0 0 0 1 1 1 1 0 ...\n $ Stroke              : num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n $ HeartDiseaseorAttack: num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ PhysActivity        : num [1:253680] 0 1 0 1 1 1 0 1 0 0 ...\n $ Fruits              : num [1:253680] 0 0 1 1 1 1 0 0 1 0 ...\n $ Veggies             : num [1:253680] 1 0 0 1 1 1 0 1 1 1 ...\n $ HvyAlcoholConsump   : num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n $ AnyHealthcare       : num [1:253680] 1 0 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : num [1:253680] 0 1 1 0 0 0 0 0 0 0 ...\n $ GenHlth             : num [1:253680] 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num [1:253680] 18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num [1:253680] 15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : num [1:253680] 1 0 1 0 0 0 0 1 1 0 ...\n $ Sex                 : num [1:253680] 0 0 0 0 0 1 0 0 0 1 ...\n $ Age                 : num [1:253680] 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : num [1:253680] 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : num [1:253680] 3 1 8 6 4 8 7 4 1 3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Diabetes_binary = col_double(),\n  ..   HighBP = col_double(),\n  ..   HighChol = col_double(),\n  ..   CholCheck = col_double(),\n  ..   BMI = col_double(),\n  ..   Smoker = col_double(),\n  ..   Stroke = col_double(),\n  ..   HeartDiseaseorAttack = col_double(),\n  ..   PhysActivity = col_double(),\n  ..   Fruits = col_double(),\n  ..   Veggies = col_double(),\n  ..   HvyAlcoholConsump = col_double(),\n  ..   AnyHealthcare = col_double(),\n  ..   NoDocbcCost = col_double(),\n  ..   GenHlth = col_double(),\n  ..   MentHlth = col_double(),\n  ..   PhysHlth = col_double(),\n  ..   DiffWalk = col_double(),\n  ..   Sex = col_double(),\n  ..   Age = col_double(),\n  ..   Education = col_double(),\n  ..   Income = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\nConvert all binary/categorical variables to factors with meaningful level names.\n\n\ndiabetes_data &lt;- diabetes_data |&gt;\n  mutate(\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c(\"No_Diabetes\", \"Diabetes\")),\n    HighBP = factor(HighBP, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HighChol = factor(HighChol, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    CholCheck = factor(CholCheck, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Stroke = factor(Stroke, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Fruits = factor(Fruits, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Veggies = factor(Veggies, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    GenHlth = factor(GenHlth, levels = 1:5, labels = c(\"Excellent\", \"Very_Good\", \"Good\", \"Fair\", \"Poor\")),\n    DiffWalk = factor(DiffWalk, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Sex = factor(Sex, levels = c(0, 1), labels = c(\"Female\", \"Male\")),\n    Age =factor(Age, levels = 1:13, labels = c(\"18_24\", \"25_29\", \"30_34\", \"35_39\", \"40_44\", \"45_49\", \n                                               \"50_54\", \"55_59\", \"60_64\", \"65_69\", \"70_74\", \"75_79\", \"80_or_older\")),\n    Education = factor(Education, levels = 1:6, labels = c(\"Never_Attended\", \"Elementary\", \"Some_High_School\", \n                                                           \"High_School_Graduate\", \"Some_College\", \n                                                           \"College_Graduate\")),\n    Income = factor(Income, levels = 1:8, labels = c(\"Less_than_10k\", \"10k_to_15k\", \n                                                     \"15k_to_20k\", \"20k_to_25k\", \n                                                     \"25k_to_35k\", \"35k_to_50k\", \n                                                     \"50k_to_75k\", \"75k_or_more\"))\n  )\n\nstr(diabetes_data)\n\ntibble [253,680 × 22] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary     : Factor w/ 2 levels \"No_Diabetes\",..: 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol            : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 2 2 1 2 2 1 ...\n $ CholCheck           : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits              : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies             : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AnyHealthcare       : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 2 2 2 2 2 2 2 ...\n $ NoDocbcCost         : Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 1 1 1 1 1 1 1 ...\n $ GenHlth             : Factor w/ 5 levels \"Excellent\",\"Very_Good\",..: 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num [1:253680] 18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num [1:253680] 15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : Factor w/ 2 levels \"No\",\"Yes\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age                 : Factor w/ 13 levels \"18_24\",\"25_29\",..: 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : Factor w/ 6 levels \"Never_Attended\",..: 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : Factor w/ 8 levels \"Less_than_10k\",..: 3 1 8 6 4 8 7 4 1 3 ...\n\n\n\nDetermine Rate of Missing Values Below we find that there are no missing values.\n\n\nsum_na &lt;- function(column){\n sum(is.na(column))\n}\nna_counts &lt;- diabetes_data |&gt;\n summarize(across(everything(), sum_na))\nna_counts\n\n# A tibble: 1 × 22\n  Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n            &lt;int&gt;  &lt;int&gt;    &lt;int&gt;     &lt;int&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n1               0      0        0         0     0      0      0\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;int&gt;, PhysActivity &lt;int&gt;,\n#   Fruits &lt;int&gt;, Veggies &lt;int&gt;, HvyAlcoholConsump &lt;int&gt;, AnyHealthcare &lt;int&gt;,\n#   NoDocbcCost &lt;int&gt;, GenHlth &lt;int&gt;, MentHlth &lt;int&gt;, PhysHlth &lt;int&gt;,\n#   DiffWalk &lt;int&gt;, Sex &lt;int&gt;, Age &lt;int&gt;, Education &lt;int&gt;, Income &lt;int&gt;"
  },
  {
    "objectID": "EDA.html#summarizations",
    "href": "EDA.html#summarizations",
    "title": "EDA of Diabetes Health Indicators Dataset",
    "section": "Summarizations",
    "text": "Summarizations\n\nThis section contains meaningful summary statistics and plots about the data (especially as it relates to the response).\n\n\nDescribe numerical data - Below are summary statistics, such as count, mean, sd, etc., for the three numerical variables in the data set.\n\n\nnumeric_data &lt;- diabetes_data |&gt;\n  select_if(is.numeric)\npsych::describe(numeric_data)\n\n         vars      n  mean   sd median trimmed  mad min max range skew kurtosis\nBMI         1 253680 28.38 6.61     27   27.68 4.45  12  98    86 2.12    11.00\nMentHlth    2 253680  3.18 7.41      0    1.04 0.00   0  30    30 2.72     6.44\nPhysHlth    3 253680  4.24 8.72      0    1.77 0.00   0  30    30 2.21     3.50\n           se\nBMI      0.01\nMentHlth 0.01\nPhysHlth 0.02\n\n\n\nBelow are contingency tables for the factor variables with respect to our Diabetes_binary response variable.\n\n\ncategorical_data &lt;- diabetes_data |&gt;\n  select_if(is.factor) |&gt;\n  select(-Diabetes_binary)\n\nfor (var in names(categorical_data)) {\n  print(var)\n  print(table(categorical_data[[var]], diabetes_data$Diabetes_binary))\n}\n\n[1] \"HighBP\"\n     \n      No_Diabetes Diabetes\n  No       136109     8742\n  Yes       82225    26604\n[1] \"HighChol\"\n     \n      No_Diabetes Diabetes\n  No       134429    11660\n  Yes       83905    23686\n[1] \"CholCheck\"\n     \n      No_Diabetes Diabetes\n  No         9229      241\n  Yes      209105    35105\n[1] \"Smoker\"\n     \n      No_Diabetes Diabetes\n  No       124228    17029\n  Yes       94106    18317\n[1] \"Stroke\"\n     \n      No_Diabetes Diabetes\n  No       211310    32078\n  Yes        7024     3268\n[1] \"HeartDiseaseorAttack\"\n     \n      No_Diabetes Diabetes\n  No       202319    27468\n  Yes       16015     7878\n[1] \"PhysActivity\"\n     \n      No_Diabetes Diabetes\n  No        48701    13059\n  Yes      169633    22287\n[1] \"Fruits\"\n     \n      No_Diabetes Diabetes\n  No        78129    14653\n  Yes      140205    20693\n[1] \"Veggies\"\n     \n      No_Diabetes Diabetes\n  No        39229     8610\n  Yes      179105    26736\n[1] \"HvyAlcoholConsump\"\n     \n      No_Diabetes Diabetes\n  No       204910    34514\n  Yes       13424      832\n[1] \"AnyHealthcare\"\n     \n      No_Diabetes Diabetes\n  No        10995     1422\n  Yes      207339    33924\n[1] \"NoDocbcCost\"\n     \n      No_Diabetes Diabetes\n  No       200722    31604\n  Yes       17612     3742\n[1] \"GenHlth\"\n           \n            No_Diabetes Diabetes\n  Excellent       44159     1140\n  Very_Good       82703     6381\n  Good            62189    13457\n  Fair            21780     9790\n  Poor             7503     4578\n[1] \"DiffWalk\"\n     \n      No_Diabetes Diabetes\n  No       188780    22225\n  Yes       29554    13121\n[1] \"Sex\"\n        \n         No_Diabetes Diabetes\n  Female      123563    18411\n  Male         94771    16935\n[1] \"Age\"\n             \n              No_Diabetes Diabetes\n  18_24              5622       78\n  25_29              7458      140\n  30_34             10809      314\n  35_39             13197      626\n  40_44             15106     1051\n  45_49             18077     1742\n  50_54             23226     3088\n  55_59             26569     4263\n  60_64             27511     5733\n  65_69             25636     6558\n  70_74             18392     5141\n  75_79             12577     3403\n  80_or_older       14154     3209\n[1] \"Education\"\n                      \n                       No_Diabetes Diabetes\n  Never_Attended               127       47\n  Elementary                  2860     1183\n  Some_High_School            7182     2296\n  High_School_Graduate       51684    11066\n  Some_College               59556    10354\n  College_Graduate           96925    10400\n[1] \"Income\"\n               \n                No_Diabetes Diabetes\n  Less_than_10k        7428     2383\n  10k_to_15k           8697     3086\n  15k_to_20k          12426     3568\n  20k_to_25k          16081     4054\n  25k_to_35k          21379     4504\n  35k_to_50k          31179     5291\n  50k_to_75k          37954     5265\n  75k_or_more         83190     7195\n\n\n\n\n\n\nfor (var in names(categorical_data)) {\n  p &lt;- ggplot(diabetes_data, aes_string(x = var, fill = \"Diabetes_binary\")) +\n    geom_bar(position = \"dodge\") +\n    labs(title = paste(\"Bar Plot of\", var, \"by Diabetes Status\"), x = var, y = \"Count\") +\n    theme_minimal()\n  print(p)\n}\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBelow are histograms of our numeric data\n\n\nfor (var in names(numeric_data)) {\n  p &lt;- ggplot(diabetes_data, aes_string(x = var)) + \n    geom_histogram(binwidth = 1, fill = \"blue\", color = \"black\", alpha = 0.7) +\n    labs(title = paste(\"Histogram of\", var), x = var, y = \"Frequency\") +\n    theme_minimal()\n  print(p)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we have a correlation matrix of our numeric data\n\n\ncor(numeric_data)\n\n                BMI   MentHlth  PhysHlth\nBMI      1.00000000 0.08531016 0.1211411\nMentHlth 0.08531016 1.00000000 0.3536189\nPhysHlth 0.12114111 0.35361887 1.0000000"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling of Diabetes Health Indicators Dataset",
    "section": "",
    "text": "The response variable we will be modeling is Diabetes_binary. It has responses of 0 for “no diabetes” and 1 for “diabetes”. The goal of modeling below will be to see which of the other variables are good at predicting diabetes, as well as which model performs the best.\n\n\n# All libraries needed\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(gbm)"
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "Modeling of Diabetes Health Indicators Dataset",
    "section": "",
    "text": "The response variable we will be modeling is Diabetes_binary. It has responses of 0 for “no diabetes” and 1 for “diabetes”. The goal of modeling below will be to see which of the other variables are good at predicting diabetes, as well as which model performs the best.\n\n\n# All libraries needed\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(gbm)"
  },
  {
    "objectID": "Modeling.html#split-into-training-and-test-sets",
    "href": "Modeling.html#split-into-training-and-test-sets",
    "title": "Modeling of Diabetes Health Indicators Dataset",
    "section": "Split into Training and Test Sets",
    "text": "Split into Training and Test Sets\n\nBelow we will split the data into a training (70% of the data) and a test set (30% of the data), and will use set.seed() to make things reproducible.\n\n\nset.seed(123)\ntrainIndex &lt;- createDataPartition(diabetes_data$Diabetes_binary, p = 0.7, list = FALSE)\ndiabetesTrain &lt;- diabetes_data[trainIndex, ]\ndiabetesTest &lt;- diabetes_data[-trainIndex, ]"
  },
  {
    "objectID": "Modeling.html#logloss",
    "href": "Modeling.html#logloss",
    "title": "Modeling of Diabetes Health Indicators Dataset",
    "section": "logLoss",
    "text": "logLoss\n\nThe goal is to create models for predicting the Diabetes_binary variable (using caret). We’ll use logLoss as our metric to evaluate models. For all model types use logLoss with 3 fold cross-validation to select the best model. You should set up your own grid of tuning parameters in any model where that is possible.\n\n\n“Log Loss is a logarithmic transformation of the likelihood function, primarily used to evaluate the performance of probabilistic classifiers. Unlike other metrics such as accuracy, Log Loss takes into account the uncertainty of predictions by penalizing models more heavily for confidently incorrect predictions. Log Loss is widely used in binary and multiclass classification problems, particularly when dealing with models that output probability scores for each class (e.g., logistic regression, neural networks). It encourages models to be not only accurate but also confident in their predictions, making it particularly useful in scenarios where misclassification costs are uneven. A lower Log Loss indicates better model performance.”\n\n\n\nSource: https://medium.com/@TheDataScience-ProF/understanding-log-loss-a-comprehensive-guide-with-code-examples-c79cf5411426"
  },
  {
    "objectID": "Modeling.html#logistic-regression-models",
    "href": "Modeling.html#logistic-regression-models",
    "title": "Modeling of Diabetes Health Indicators Dataset",
    "section": "Logistic Regression Models",
    "text": "Logistic Regression Models\n\nLogistic regression is used to model a response that is success or fail. It is a type of regression analysis used for predicting the outcome of a categorical dependent variable based on one or more predictor variables. The log-odds of the dependent variable being 1 (for a binary outcome) is modeled as a linear combination of the independent variables and the output of the model is always between 0 and 1.\n\n\nLogistic regression makes sense as a model for our diabetes data set since the response variable, Diabetes_binary, is binary.\n\n\nBelow are three candidate logistic regression models.\n\n\n# First Logistic Regression model\nlog_reg1 &lt;- train(Diabetes_binary ~ HighBP + HighChol, \n                  data = diabetesTrain,\n                  method = \"glm\",\n                  family = \"binomial\",\n                  metric = \"logLoss\",\n                  trControl = trainControl(method = \"cv\", number = 5, \n                                           summaryFunction = mnLogLoss, \n                                           classProbs = TRUE,\n                                           savePredictions = \"final\"))\n\n# Second Logistic Regression model\nlog_reg2 &lt;- train(Diabetes_binary ~ BMI + Age + HighBP + HighChol, \n                  data = diabetesTrain,\n                  method = \"glm\",\n                  family = \"binomial\",\n                  metric = \"logLoss\",\n                  trControl = trainControl(method = \"cv\", number = 5, \n                                           summaryFunction = mnLogLoss, \n                                           classProbs = TRUE,\n                                           savePredictions = \"final\"))\n\n# Third Logistic Regression model\nlog_reg3 &lt;- train(Diabetes_binary ~ BMI + Age + HighBP*HighChol, \n                  data = diabetesTrain,\n                  method = \"glm\",\n                  family = \"binomial\",\n                  metric = \"logLoss\",\n                  trControl = trainControl(method = \"cv\", number = 5, \n                                           summaryFunction = mnLogLoss, \n                                           classProbs = TRUE,\n                                           savePredictions = \"final\"))\n\nprint(log_reg1)\n\nGeneralized Linear Model \n\n177577 samples\n     2 predictor\n     2 classes: 'No_Diabetes', 'Diabetes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142061, 142063, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.3612394\n\nprint(log_reg2)\n\nGeneralized Linear Model \n\n177577 samples\n     4 predictor\n     2 classes: 'No_Diabetes', 'Diabetes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142062, 142061, 142062, 142061 \nResampling results:\n\n  logLoss  \n  0.3408505\n\nprint(log_reg3)\n\nGeneralized Linear Model \n\n177577 samples\n     4 predictor\n     2 classes: 'No_Diabetes', 'Diabetes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142062, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.3408803\n\n\n\nOur second and third logistic models have the smallest logLoss and their values are essentially the same. Since the second model is the less complicated model of the two, we will choose that model as our best logistic regression model."
  },
  {
    "objectID": "Modeling.html#classification-tree",
    "href": "Modeling.html#classification-tree",
    "title": "Modeling of Diabetes Health Indicators Dataset",
    "section": "Classification Tree",
    "text": "Classification Tree\n\nA classification tree splits up predictor space into regions and there are different predictions for each region. The goal is to classify (predict) group membership. For a given region, you usually use most prevalent class as prediction. This model might work here since we can consider no diabetes and diabetes as the classes for prediction.\n\n\nBelow is the fit for a classification tree with varying values for the complexity parameter.\n\n\nct_model &lt;- train(Diabetes_binary ~ BMI + Age + HighBP + HighChol, \n                    data = diabetesTrain, \n                    method = \"rpart\",\n                    metric = \"logLoss\",\n                    tuneGrid = expand.grid(cp = seq(0.001, 0.005, by = 0.001)),\n                    trControl = trainControl(method = \"cv\", \n                                             number = 5, \n                                             summaryFunction = mnLogLoss, \n                                             classProbs = TRUE,savePredictions = \"final\"))\n\nct_model\n\nCART \n\n177577 samples\n     4 predictor\n     2 classes: 'No_Diabetes', 'Diabetes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142061, 142061, 142063, 142062 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.001  0.3590938\n  0.002  0.4037576\n  0.003  0.4037576\n  0.004  0.4037576\n  0.005  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.001.\n\n\n\nThe best model (best complexity parameter) based on logLoss is cp = 0.001."
  },
  {
    "objectID": "Modeling.html#random-forest",
    "href": "Modeling.html#random-forest",
    "title": "Modeling of Diabetes Health Indicators Dataset",
    "section": "Random Forest",
    "text": "Random Forest\n\nRandom Forest is an ensemble learning method that creates multiple trees from bootstrap samples and averages the results to create a more robust and accurate model. In this model, you don’t use all predictors, and instead use a random subset of predictors for each bootstrap sample/tree fit. By randomly selecting a subset of predictors, a good predictor or two won’t dominate the tree fits the way it would in a classification tree.\n\n\nrf_model &lt;- train(Diabetes_binary ~ BMI + Age + HighBP + HighChol, \n                  data = diabetesTrain, \n                  method = \"rf\",\n                  metric = \"logLoss\",\n                  tuneGrid = expand.grid(mtry = ncol(diabetesTrain)/3),\n                  trControl = trainControl(method = \"cv\", \n                                           number = 3, \n                                           summaryFunction = mnLogLoss, \n                                           classProbs = TRUE,\n                                           savePredictions = \"final\"),\n                  ntree = 100)\nprint(rf_model)\n\nRandom Forest \n\n177577 samples\n     4 predictor\n     2 classes: 'No_Diabetes', 'Diabetes' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 118386, 118384, 118384 \nResampling results:\n\n  logLoss \n  3.711764\n\nTuning parameter 'mtry' was held constant at a value of 7.333333"
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "Modeling of Diabetes Health Indicators Dataset",
    "section": "Final Model Selection",
    "text": "Final Model Selection\n\nWe now have three best models (one for each model type above). Now we will compare all three models on the test set and declare an overall winner!\n\n\n# Calculate predictions for each model\nlog_preds &lt;- predict(log_reg2, newdata = diabetesTest, type='prob')\nct_preds &lt;- predict(ct_model, newdata = diabetesTest, type='prob')\nrf_preds &lt;- predict(rf_model, newdata = diabetesTest, type='prob')\n\n\nModelMetrics::logLoss(diabetesTest$Diabetes_binary, log_preds[,2])\n\n[1] 2.620735\n\nModelMetrics::logLoss(diabetesTest$Diabetes_binary, ct_preds[,2])\n\n[1] 2.429865\n\nModelMetrics::logLoss(diabetesTest$Diabetes_binary, rf_preds[,2])\n\n[1] 36.15307\n\n\n\nBased on the logLoss metric we would choose the classification tree model in this case."
  }
]